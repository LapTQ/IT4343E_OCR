{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/LapTQ/handwritten_text_recognition/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Open in Colab](https://colab.research.google.com/github/LapTQ/handwritten_text_recognition/blob/main/demo.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8snhH11sXOxU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://excalidraw.com/#room=da5fbbdb8ae8e17a0e8d,rLBfY6UNZueKq4VnvKFRZA\n",
    "https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image alignment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def process_for_contour(img, **kwargs):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # XỬ LÝ ẢNH: làm mờ ảnh để tập trung vào phần khung giấy\n",
    "    if 'median_ksize' not in kwargs:\n",
    "        kwargs['median_ksize'] = 15\n",
    "    img_blur = cv2.medianBlur(img_gray, kwargs['median_ksize'])\n",
    "\n",
    "    # ### START DEMO\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(img_blur, cmap='gray')\n",
    "    # plt.show()\n",
    "    # ### END DEMO\n",
    "\n",
    "    if 'mode' not in kwargs:\n",
    "        kwargs['mode'] = 'thresh'\n",
    "    assert kwargs['mode'] == 'thresh' or kwargs['mode'] == 'edge'\n",
    "\n",
    "    if kwargs['mode'] == 'thresh':\n",
    "        if 'use_page_median' not in kwargs:\n",
    "            kwargs['use_page_median'] = True\n",
    "\n",
    "        if kwargs['use_page_median']:\n",
    "            H, W = img_blur.shape[:2]\n",
    "            lower, upper = 0.4, 0.6\n",
    "            page_median = np.median(img_blur[int(lower * H):int(upper * H), int(lower * W):int(upper * W)])\n",
    "            thresh1, thresh2 = max(1, page_median - 10), min(page_median + 10, 254)\n",
    "            ret, thresh1 = cv2.threshold(img_blur, thresh1, 255, cv2.THRESH_BINARY)\n",
    "            ret, thresh2 = cv2.threshold(255 - img_blur, 255 - thresh2, 255, cv2.THRESH_BINARY)\n",
    "            out_img = np.logical_and(thresh1, thresh2).astype(np.uint8)\n",
    "        else:\n",
    "            ret, out_img = cv2.threshold(img_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        if 'threshold1' not in kwargs:\n",
    "            kwargs['threshold1'] = 100\n",
    "        if 'threshold2' not in kwargs:\n",
    "            kwargs['threshold2'] = 200\n",
    "        out_img = cv2.Canny(img_blur, kwargs['threshold1'], kwargs['threshold2'])\n",
    "\n",
    "    # ### START DEMO\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(out_img, cmap='gray')\n",
    "    # plt.show()\n",
    "    # ### END DEMO\n",
    "\n",
    "    return out_img\n",
    "\n",
    "def auto_detect_corners(processed_img):\n",
    "    \"\"\"\n",
    "    Get 4 corner-coordinates of scanned document in a preprocessed grayscale image.\n",
    "    :param processed_img: gray-scale np-array image\n",
    "    :return: np-array shape (4, 1, 2) coordinates of top-left, bottom-left, bottom-right, top-right corners\n",
    "    \"\"\"\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "        image=processed_img,\n",
    "        mode=cv2.RETR_CCOMP,\n",
    "        method=cv2.CHAIN_APPROX_NONE\n",
    "    )\n",
    "\n",
    "    # xấp xỉ đa giác\n",
    "    epsilon = [0.1 * cv2.arcLength(cnt, True) for cnt in contours]\n",
    "    quad_approx = [cv2.approxPolyDP(cnt, ep, True) for cnt, ep in zip(contours, epsilon)]\n",
    "\n",
    "    # làm sao dể biết contour nào là của giấy?\n",
    "    # tạm thời dựa vào diện tích\n",
    "    H, W = processed_img.shape[:2]\n",
    "    for corners in quad_approx:\n",
    "        if cv2.contourArea(corners) >= H * W / 10 and len(corners) == 4:\n",
    "            return corners\n",
    "\n",
    "def corner_warping(img, corners):\n",
    "    \"\"\"\n",
    "\n",
    "    :param img: np-array image\n",
    "    :param corners: (x, y) coordinates of top-left, bottom-left, bottom-right, top-right corners\n",
    "    :return: warped image\n",
    "    \"\"\"\n",
    "    assert corners.shape == (4, 1, 2), f\"shape should be (4, 1, 2) for the convenience of OpenCV. Got {corners.shape}\"\n",
    "\n",
    "    # tìm box (theo trục): xywh (xy top-left)\n",
    "    rect = cv2.boundingRect(corners)\n",
    "    x1, y1, w, h = rect\n",
    "\n",
    "    # crop ảnh và tính lại tọa độ các góc\n",
    "    img = img[y1:y1 + h, x1:x1 + w]\n",
    "    corners[:, :, 0] = corners[:, :, 0] - x1\n",
    "    corners[:, :, 1] = corners[:, :, 1] - y1\n",
    "    # tọa độ warp bằng 4 góc của ảnh đã crop\n",
    "    target_coord = np.array([[0, 0], [0, h], [w, h], [w, 0]]).reshape(corners.shape)\n",
    "\n",
    "    homography, mask = cv2.findHomography(corners, target_coord, cv2.RANSAC)    # hoặc dùng hàm cv2.getPerspectiveTransform\n",
    "    warped_img = cv2.warpPerspective(img, homography, (w, h))\n",
    "\n",
    "    return warped_img\n",
    "\n",
    "def suggest_corner(img_shape):\n",
    "    H, W = img_shape[:2]\n",
    "    return np.array([[int(W * 0.25), int(H * 0.25)],\n",
    "                     [int(W * 0.25), int(H * 0.75)],\n",
    "                     [int(W * 0.75), int(H * 0.75)],\n",
    "                     [int(W * 0.75), int(H * 0.25)]]).reshape(4, 1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def template_alignment(img, template):\n",
    "    \"\"\"\n",
    "\n",
    "    :param img: np-array in OpenCV BGR format\n",
    "    :param template: np-array in OpenCV BGR format\n",
    "    :return: np-array in OpenCV BGR format\n",
    "    \"\"\"\n",
    "\n",
    "    # chuyển về ảnh grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # sử dụng SIFT descriptor\n",
    "    detector = cv2.SIFT_create()\n",
    "    kp_img, des_img = detector.detectAndCompute(img_gray, None)\n",
    "    kp_template, des_template = detector.detectAndCompute(template_gray, None)\n",
    "\n",
    "    # tìm các cặp bằng Brute force và KNN\n",
    "    matcher = cv2.BFMatcher()\n",
    "    matches = matcher.knnMatch(des_img, des_template, k=2)\n",
    "\n",
    "    # lấy ra các cặp tốt (theo bài báo của SIFT)\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    # trựa quan hóa các cặp (không có ích gì cho hàm)\n",
    "    matching_img = cv2.drawMatches(img_gray, kp_img, template_gray, kp_template, good, None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # tạo biến lưu tọa độ các cặp\n",
    "    img_points = np.zeros((len(good), 2), dtype=np.float32)\n",
    "    template_points = np.zeros((len(good), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(good):\n",
    "        img_points[i, :] = kp_img[match.queryIdx].pt\n",
    "        template_points[i, :] = kp_template[match.trainIdx].pt\n",
    "\n",
    "    homography, mask = cv2.findHomography(img_points, template_points, cv2.RANSAC)\n",
    "\n",
    "    H, W, D = template.shape\n",
    "    img_reg = cv2.warpPerspective(img, homography, (W, H))\n",
    "\n",
    "    return img_reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def arrange_corners(corners):\n",
    "    shape = corners.shape\n",
    "    corners = np.squeeze(corners).tolist()\n",
    "    corners = sorted(corners, key=lambda x: x[0])\n",
    "    corners = sorted(corners[:2], key=lambda x: x[1]) + sorted(corners[2:], key=lambda x: x[1], reverse=True)\n",
    "    return np.array(corners).reshape(shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "img_path = 'img2.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "use_corner = True\n",
    "\n",
    "if input('Do you have template? [y/n] ').strip()[0].lower() == 'y':\n",
    "    # TODO: cho người dùng chọn template ở đây\n",
    "    template_path = 'template.jpg'\n",
    "    template = cv2.imread(template_path)\n",
    "\n",
    "    img_reg = template_alignment(img, template)\n",
    "\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(cv2.cvtColor(img_reg, cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "    cv2.namedWindow('Preview')\n",
    "    cv2.imshow('Preview', img_reg)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    use_corner = input('Alignment using template. Is this okay? [y/n] ').strip()[0].lower() != 'y'\n",
    "\n",
    "\n",
    "if use_corner:\n",
    "    processed_img = process_for_contour(img, mode='thresh', use_page_median=True)\n",
    "    corners = auto_detect_corners(processed_img)\n",
    "    if corners is None:\n",
    "        processed_img = process_for_contour(img, mode='thresh', use_page_median=False)\n",
    "        corners = auto_detect_corners(processed_img)\n",
    "    if corners is None:\n",
    "        processed_img = process_for_contour(img, mode='edge')\n",
    "        corners = auto_detect_corners(processed_img)\n",
    "\n",
    "    if corners is None:\n",
    "        corners = suggest_corner(img.shape)\n",
    "\n",
    "    demo_img = img.copy()\n",
    "    cv2.drawContours(\n",
    "        image=demo_img,\n",
    "        contours=[corners],\n",
    "        contourIdx=-1,\n",
    "        color=(0, 255, 0),\n",
    "        thickness=2,\n",
    "        lineType=cv2.LINE_AA\n",
    "    )\n",
    "    for center in corners:\n",
    "        center = center[0]\n",
    "        cv2.circle(\n",
    "            demo_img,\n",
    "            center,\n",
    "            radius=2,\n",
    "            color=(0, 0, 255),\n",
    "            thickness=2\n",
    "        )\n",
    "\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(cv2.cvtColor(demo_img, cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "    cv2.namedWindow('Auto detect corners preview')\n",
    "    cv2.imshow('Auto detect corners preview', demo_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if input('Auto detect corners. Is this okay? [y/n] ').strip()[0].lower() != 'y':\n",
    "\n",
    "        global img_copy\n",
    "        global buffer\n",
    "\n",
    "        def on_mouse(event, x, y, flags, param):\n",
    "            global buffer\n",
    "            global img_copy\n",
    "\n",
    "            if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                cv2.circle(img_copy, (x, y), radius=4, color=(0, 0, 255), thickness=8)\n",
    "                if len(buffer) > 0:\n",
    "                    cv2.line(img_copy, (x, y), buffer[-1], color=(0, 255, 0), thickness=2)\n",
    "                if len(buffer) == 3:\n",
    "                    cv2.line(img_copy, (x, y), buffer[0], color=(0, 255, 0), thickness=2)\n",
    "                buffer.append([x, y])\n",
    "                cv2.imshow('Please specify corners', img_copy)\n",
    "\n",
    "\n",
    "        cv2.namedWindow('Please specify corners')\n",
    "        img_copy = img.copy()\n",
    "        cv2.imshow('Please specify corners', img_copy)\n",
    "\n",
    "        corner_selected = False\n",
    "\n",
    "        while not corner_selected:\n",
    "            img_copy = img.copy()\n",
    "            buffer = []\n",
    "            cv2.imshow('Please specify corners', img_copy)\n",
    "            cv2.setMouseCallback('Please specify corners', on_mouse)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            corner_selected = input('Are you sure with your choice? [y/n] ').strip()[0].lower() == 'y'\n",
    "\n",
    "        # TODO viết hàm sắp xếp các góc\n",
    "        corners = np.array(buffer).reshape(4, 1, 2)\n",
    "        corners = arrange_corners(corners)\n",
    "\n",
    "    img_reg = corner_warping(img, corners)\n",
    "\n",
    "# print('Final alignment is:')\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(cv2.cvtColor(img_reg, cv2.COLOR_BGR2RGB))\n",
    "# plt.show()\n",
    "if use_corner:\n",
    "    cv2.namedWindow('Final alignment')\n",
    "    cv2.imshow('Final alignment', img_reg)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess for text detection\n",
    "\n",
    "Nếu dùng mạng neuron để detect thì có cần tiền xử lý không, hay là vẫn detect được rồi xử lý sau?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://pyimagesearch.com/2017/02/20/text-skew-correction-opencv-python/\n",
    "https://medium.com/technovators/survey-on-image-preprocessing-techniques-to-improve-ocr-accuracy-616ddb931b76\n",
    "https://medium.com/analytics-vidhya/enhance-a-document-scan-using-python-and-opencv-9934a0c2da3d\n",
    "https://repository.vnu.edu.vn/bitstream/VNU_123/6297/1/00050001352.pdf\n",
    "https://arxiv.org/abs/1901.06081\n",
    "\n",
    "blur correction\n",
    "\n",
    "remove noise\n",
    "\n",
    "Deskew\n",
    "\n",
    "Lines Straightening\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = img_reg\n",
    "\n",
    "img_hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "# hist = cv2.calcHist(img_hls, [1], None, [256], [0, 256])\n",
    "# plt.plot(hist)\n",
    "# plt.show()\n",
    "#\n",
    "# x = np.linspace(0, 255, 256)\n",
    "# def f(x, k, m):\n",
    "#     return int(255 / (1 + np.exp(-k * (x - m))))\n",
    "#\n",
    "# fv = np.vectorize(lambda x: f(x, 0.1, 75))\n",
    "# y = fv(x)\n",
    "# plt.plot(x, y)\n",
    "# plt.show()\n",
    "#\n",
    "# img_hls[:, :, 1] = fv(img_hls[:, :, 1])\n",
    "\n",
    "img_hls[:, :, 1] = cv2.adaptiveThreshold(\n",
    "    img_hls[:, :, 1],\n",
    "    55,  # maximum value assigned to pixel values exceeding the threshold\n",
    "    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,  # gaussian weighted sum of neighborhood\n",
    "    cv2.THRESH_BINARY,  # thresholding type\n",
    "    15,  # block size (5x5 window)\n",
    "    3\n",
    ")\n",
    "\n",
    "img = cv2.cvtColor(img_hls, cv2.COLOR_HLS2BGR)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text detection\n",
    "CRAFT\n",
    "DB\n",
    "\n",
    "Text recognition\n",
    "https://github.com/clovaai/deep-text-recognition-benchmark\n",
    "\n",
    "\n",
    "Text spotting\n",
    "FOTS\n",
    "ABCNet\n",
    "Mask TextSpotter\n",
    "\n",
    "Information extraction\n",
    "https://viblo.asia/p/information-extraction-trong-ocr-la-gi-phuong-phap-nao-de-giai-quyet-bai-toan-yMnKMjzmZ7P\n",
    "PICK (https://arxiv.org/abs/2004.07464)\n",
    "\n",
    "\n",
    "Example\n",
    "https://github.com/mindee/doctr\n",
    "https://viblo.asia/p/bai-toan-trich-xuat-thong-tin-tu-hoa-don-ORNZqd4nK0n\n",
    "\n",
    "data\n",
    "icdar 2019 SROIE (sự kiện này có nhiều dataset hay)\n",
    "https://paperswithcode.com/paper/190513538\n",
    "https://paperswithcode.com/dataset/ddi-100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# img_blur = cv2.medianBlur(img_gray, 15)\n",
    "#\n",
    "# H, W = img_blur.shape[:2]\n",
    "# lower, upper = 0.4, 0.6\n",
    "# page_median = np.median(img_blur[int(lower * H):int(upper * H), int(lower * W):int(upper * W)])\n",
    "# print(page_median)\n",
    "#\n",
    "# hist = cv2.calcHist([img_blur], [0], None, [256], [0, 256])\n",
    "# hist = hist / (H * W)\n",
    "# plt.plot(hist)\n",
    "# plt.show()\n",
    "#\n",
    "# # chọn dựa trên khoảng cách và tấn suất (liệu có cần tần suất không?)\n",
    "# scores = np.zeros_like(hist)\n",
    "# for i in range(256):\n",
    "#     scores[i] = 0.05 * 1/(np.abs(page_median - i) + 1) + hist[i]\n",
    "# plt.plot(scores)\n",
    "# plt.show()\n",
    "#\n",
    "# page_color = np.argmax(scores)\n",
    "# threshold1, threshold2 = max(0, page_color - 10), min(page_color + 20, 220)\n",
    "#\n",
    "# ret, thresh1 = cv2.threshold(img_blur, threshold1, 255, cv2.THRESH_BINARY)\n",
    "# ret, thresh2 = cv2.threshold(255 - img_blur, 255 - threshold2, 255, cv2.THRESH_BINARY)\n",
    "# ret, thresh = cv2.threshold(img_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(thresh, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# peaks = []\n",
    "# wsize = 11\n",
    "# hist_pad = np.concatenate([np.zeros(((wsize - 1)//2, 1), dtype=np.float32),\n",
    "#                            hist,\n",
    "#                            np.zeros(((wsize - 1)//2, 1), dtype=np.float32)],\n",
    "#                           axis=0)\n",
    "# for i in range((wsize - 1)//2, len(hist) - (wsize - 1)//2):\n",
    "\n",
    "\n",
    "# gaussian_smooth = np.array([.006, .061, .242, .383, .242, .061, .006])\n",
    "# hist = np.concatenate([np.zeros((3,1), dtype=np.float32), hist, np.zeros((3,1), dtype=np.float32)], axis=0)\n",
    "# for i in range(3, len(hist) - 3):\n",
    "#     hist[i, 0] = np.sum(hist[i - 3: i + 4, 0] * gaussian_smooth)\n",
    "# hist = hist[3:-3]\n",
    "#\n",
    "# plt.plot(hist)\n",
    "# plt.show()\n",
    "#\n",
    "# demo_img = img_blur"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "img_path = 'img2.png'\n",
    "img = cv2.imread(img_path)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img_blur = cv2.medianBlur(img_gray, 15)\n",
    "img_edges = cv2.Canny(img_blur, 100, 200)\n",
    "\n",
    "lines = cv2.HoughLines(img_edges, 1, np.pi/180, 150, None, 0, 0)\n",
    "for i in range(len(lines)):\n",
    "    rho = lines[i][0][0]\n",
    "    theta = lines[i][0][1]\n",
    "    a = math.cos(theta)\n",
    "    b = math.sin(theta)\n",
    "    x0 = a * rho\n",
    "    y0 = b * rho\n",
    "    pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n",
    "    pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n",
    "    cv2.line(img, pt1, pt2, (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "69qHRo43XOxc",
    "outputId": "ba31d8fe-6753-4473-a479-8b6280adce1d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/LapTQ/handwritten_text_recognition.git\n",
    "!pip install jiwer\n",
    "%cd handwritten_text_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py --invert_color True"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0cR80cqUXOxf",
    "outputId": "8c90cba0-cde0-4ede-d13f-9232f39ffdf5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "demo.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}