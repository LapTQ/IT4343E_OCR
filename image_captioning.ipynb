{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LapTQ/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hungpham13/Vietnamese-HTR.git\n",
        "\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_6PnrTXw-c",
        "outputId": "24561404-c0a3-4d57-cfdc-4a2a3f9732ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-HTR'...\n",
            "remote: Enumerating objects: 2403, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2403 (delta 0), reused 4 (delta 0), pack-reused 2399\u001b[K\n",
            "Receiving objects: 100% (2403/2403), 427.59 MiB | 36.10 MiB/s, done.\n",
            "Checking out files: 100% (2395/2395), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQiOo4_Fd7lh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from jiwer import wer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0825_DataSamples 1/labels.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgkwHIEfYniD",
        "outputId": "d4a393cd-7acd-4eca-b600-32baf7604fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"1.jpg\": \"Số 3 Nguyễn Ngọc Vũ, Hà Nội\",\n",
            "    \"2.jpg\": \"Số 30 Nguyên Hồng, Láng Hạ, Đống Đa, Hà Nội\",\n",
            "    \"3.jpg\": \"58 Thái Thịnh, Đống Đa, Hà Nội\",\n",
            "    \"4.jpeg\": \"Số 370/8 khu phố 5B, phường Tân Biên, Biên Hòa, Đồng Nai\",\n",
            "    \"5.jpg\": \"Vĩnh Trung Plaza, B, 255-257 đường Hùng Vương, phường Vĩnh Trung\",\n",
            "    \"6.jpg\": \"Tòa nhà 34T, Hoàng Đạo Thúy, Hà Nội\",\n",
            "    \"7.jpg\": \"40 Cát Linh, Đống Đa, Hà Nội\",\n",
            "    \"8.jpg\": \"phòng 101, tầng 1, lô 04-TT5B, khu đô thị Tây Nam Linh Đàm\",\n",
            "    \"9.JPG\": \"Nhà 87 ngõ 416 Đê La Thành\",\n",
            "    \"10.JPG\": \"Up coworking Space, 89 Láng Hạ, Hà Nội\",\n",
            "    \"11.jpg\": \"192 Ngô Đức Kế, quận 1, Hồ Chí Minh\",\n",
            "    \"12.jpg\": \"số 5 Công Trường Mê Linh, phường Bến Nghé, quận 1\",\n",
            "    \"13.jpg\": \"90A đường Mai Xuân Thưởng, tỉnh Gia Lai\",\n",
            "    \"14.jpg\": \"96/7/12B Phạm Văn Đồng, thành phố Pleiku\",\n",
            "    \"15.jpg\": \"168 Ngô Gia Tự, thành phố Hà Tĩnh\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0916_Data Samples 2'\n",
        "test_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/1015_Private Test'\n",
        "\n",
        "image_height, image_width = 120, 1900\n",
        "vocab_size = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "seq_length = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "embedding_dim = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "units = 512\n",
        "\n",
        "batch_size = 4\n",
        "epochs = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "mjm7Y-lYdZYT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of training images:', len(list(Path(train_img_dir).glob('*.png'))))\n",
        "print('Number of testing images:', len(list(Path(test_img_dir).glob('*.png'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8HNz-regz_",
        "outputId": "0410c1b9-14ce-44c2-c7bf-6249949ded38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 1823\n",
            "Number of testing images: 549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_json = json.load(\n",
        "    open(train_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "test_json = json.load(\n",
        "    open(test_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "train_data = {os.path.join(train_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in train_json.items()}\n",
        "test_data = {os.path.join(test_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in test_json.items()}"
      ],
      "metadata": {
        "id": "pQGy_mYAayjJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img_string = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img_string)\n",
        "\n",
        "    # resize to desired shape\n",
        "    # input is of int [0, 255], but output is of float [0, 255]\n",
        "    img = tf.image.resize_with_pad(img, image_height, image_width)\n",
        "\n",
        "    # invert color      ##############################\n",
        "    img = 255 - img\n",
        "\n",
        "    # preprocess_input accept input of type float [0, 255]\n",
        "    img = keras.applications.densenet.preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace('<', '')\n",
        "strip_chars = strip_chars.replace('>', '')\n",
        "strip_chars = strip_chars.replace('/', '')\n",
        "strip_chars = strip_chars.replace('-', '')\n",
        "\n",
        "vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length,\n",
        "    standardize=lambda label: tf.strings.regex_replace(label, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        ")\n",
        "\n",
        "vectorization.adapt(list(train_data.values()))\n",
        "\n",
        "def preprocess_input(img_path, label):\n",
        "    return decode_and_resize(img_path), vectorization(label)\n",
        "\n",
        "def make_dataset(img_paths, labels, training):\n",
        "    assert training is True or training is False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "    dataset = dataset.map(preprocess_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.prefetch(buffer_size=2000)\n",
        "    # dataset = dataset.cache()\n",
        "    if training: \n",
        "        dataset = dataset.shuffle(buffer_size=2000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = make_dataset(\n",
        "    train_data.keys(),\n",
        "    train_data.values(),\n",
        "    training = True\n",
        ")\n",
        "test_ds = make_dataset(\n",
        "    test_data.keys(),\n",
        "    test_data.values(),\n",
        "    training = False\n",
        ")"
      ],
      "metadata": {
        "id": "q0rEPI32rEMH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_ds.take(1):\n",
        "#     for image, label in zip(images, labels):\n",
        "#         plt.imshow(image)\n",
        "#         plt.show()\n",
        "#         print(label)"
      ],
      "metadata": {
        "id": "e-bul7yM1PyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "\n",
        "        self.feature_extractor = keras.applications.DenseNet121(\n",
        "            include_top=False,\n",
        "            input_shape=(image_height, image_width, 3),\n",
        "\n",
        "        )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # get the feature map from DenseNet\n",
        "        x = self.feature_extractor(x)\n",
        "        # squash the feature map from shape [f_height, f_width, f_channel]\n",
        "        # to shape [f_height x f_width, f channel]\n",
        "        x = keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
        "        # shape after fc == (f_height x f_width, embedding_dim)\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mbX4mMelNBzZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape = (batch_size, f_height x f_width, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # print('BahdanauAttention: hidden_with_time_axis.shape == ', hidden_with_time_axis.shape)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, f_height x f_width, units)\n",
        "        attention_hidden_layer = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis)\n",
        "        )\n",
        "\n",
        "        # score shape == (batch_size, f_height x f_width, 1)\n",
        "        # this gives an unnormalized score for each image feature\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # print('BahdanauAttention: score.shape == ', score.shape)\n",
        "\n",
        "        # attention_weights shape == (batch_size, f_height x f_width, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # print('BahdanauAttention: attention_weights.shape == ', attention_weights.shape)\n",
        "\n",
        "        # context vector shape after sum = (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        # print('BahdanauAttention: context_vector.shape == ', context_vector.shape)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "qhg43tIOYOzU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau vs Luong"
      ],
      "metadata": {
        "id": "yvghkvH3Ceqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # print('RNN_Decoder: context_vector.shape == ', context_vector.shape)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "wOpjbibZVo8U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vectorization.vocabulary_size())"
      ],
      "metadata": {
        "id": "GhKrVuRjEIxj",
        "outputId": "60d4fcad-5fc4-413e-94a5-05f75ae8cd94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(trues, preds):\n",
        "    loss_ = loss_object(trues, preds)\n",
        "    \n",
        "    mask = tf.cast(\n",
        "        tf.math.logical_not(tf.math.equal(trues, 0)),\n",
        "        dtype=loss_.dtype\n",
        "    )\n",
        "\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "5uYOHU4EBIuu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints\n",
        "!mkdir checkpoints/train"
      ],
      "metadata": {
        "id": "7ecufyAFCYVI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoints/train'\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    optimizer=optimizer\n",
        ")\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    ckpt,\n",
        "    checkpoint_path,\n",
        "    max_to_keep=5\n",
        ")\n",
        "start_epoch = 0"
      ],
      "metadata": {
        "id": "CP6_XTeBCSyy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "metadata": {
        "id": "4ehXoIfNWUM0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary())\n",
        "index_to_word = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "Q15fA47pj1Ak"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot = []\n",
        "wer_plot = []"
      ],
      "metadata": {
        "id": "8A1DmE6jgZUG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images, targets):\n",
        "    # print('train_step: images.shape == ', images.shape)\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=targets.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims(\n",
        "        [word_to_index('<start>')] * targets.shape[0],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(images)\n",
        "\n",
        "        for i in range(1, targets.shape[1]):\n",
        "            # print('train_step: dec_input.shape == ', dec_input.shape)\n",
        "            # print('train_step: features.shape == ', features.shape)\n",
        "            # print('train_step: hidden.shape == ', hidden.shape)\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "            loss += loss_function(targets[:, i], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targets[:, i], 1)\n",
        "    \n",
        "    total_loss = loss / targets.shape[1]\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss"
      ],
      "metadata": {
        "id": "PnvqHwevgb3J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(images):\n",
        "    # images shape == (batch_size, height, width, channels)\n",
        "\n",
        "    # features shape == (batch_size, f_heigt x f_width, f_channels)\n",
        "    features = encoder(images)\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=images.shape[0])\n",
        "\n",
        "    results = tf.expand_dims([word_to_index('<start>')] * images.shape[0], axis=1)\n",
        "\n",
        "    for i in range(seq_length):\n",
        "        predictions, hidden, attention_weights = decoder(\n",
        "            results[..., -1:], #(4, 1)\n",
        "            features,\n",
        "            hidden\n",
        "        )\n",
        "\n",
        "        predicted_id = tf.expand_dims(\n",
        "            tf.argmax(predictions, axis=-1),\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "        results = tf.concat([results, predicted_id], axis=-1) # tf.expand_dims(predicted_id, 0)\n",
        "        \n",
        "    return results\n",
        "\n",
        "def evaluate(images, targets):\n",
        "    # images shape == (batch_size, height, width, channels)\n",
        "    results = predict(images)\n",
        "    true_texts = [' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in j[1:-1] if i.numpy() != 0 and i != word_to_index('<end>')])\n",
        "        for j in targets]\n",
        "    pred_texts = [' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in j[1:] if i != word_to_index('<end>')])\n",
        "        for j in results]\n",
        "    \n",
        "    return wer(true_texts, pred_texts)"
      ],
      "metadata": {
        "id": "dd7DWlL0_B9-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "D_5c1mhXrr_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, (images, targets) in enumerate(train_ds):\n",
        "\n",
        "        batch_loss, t_loss = train_step(images, targets)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            average_batch_loss = batch_loss.numpy() / int(targets.shape[1])\n",
        "            print(f'Epoch: {epoch + 1} Batch: {batch} Loss: {average_batch_loss:.4f}')\n",
        "\n",
        "    loss_plot.append(total_loss / train_ds.cardinality().numpy())\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} Loss: {total_loss/train_ds.cardinality().numpy():.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "    plt.plot(loss_plot)\n",
        "    plt.show()\n",
        "\n",
        "    total_wer = 0\n",
        "    for batch, (images, targets) in enumerate(test_ds):\n",
        "        batch_wer = evaluate(images, targets)\n",
        "        total_wer += batch_wer\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch: {epoch + 1} Batch: {batch} WER: {batch_wer:.2f}')\n",
        "    \n",
        "    wer_plot.append(total_wer / test_ds.cardinality().numpy())\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} WER: {total_wer/test_ds.cardinality().numpy():.2f}')\n",
        "\n",
        "    plt.plot(wer_plot)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()"
      ],
      "metadata": {
        "id": "AlBO4L8gpmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_plot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "4XtBcVH3Wlfz",
        "outputId": "3cc0ccd0-c0e4-4fd1-cab1-8afbfdf466e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7320d04e10>]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save checkpoint\n",
        "!tar -czf drive/MyDrive/checkpoint/htr/checkpoints2.tar.gz ./checkpoints"
      ],
      "metadata": {
        "id": "1FKaKZpnR9Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/checkpoint/htr/loss_plot2.pkl', 'wb') as f:\n",
        "    pickle.dump(loss_plot, f)"
      ],
      "metadata": {
        "id": "gpPEqVeOUrvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jdG1LjzT_fLj",
        "outputId": "b5756e34-40b6-49dc-9236-7344b68e7af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract checkpoint archive\n",
        "!tar -xf drive/MyDrive/checkpoint/htr/checkpoints.tar.gz -C ./"
      ],
      "metadata": {
        "id": "IisMzO3217MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load checkpoint\n",
        "ckpt_path = tf.train.latest_checkpoint(checkpoint_path)\n",
        "ckpt.restore(ckpt_path)"
      ],
      "metadata": {
        "id": "AIsSSyl31_Zg",
        "outputId": "91e106bb-4447-44e8-fc60-7a28fb2be794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f644c1d2550>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/checkpoint/htr/loss_plot.pkl', 'rb') as f:\n",
        "    loss_plot = pickle.load(f)"
      ],
      "metadata": {
        "id": "ZUdR-XS9VKmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, targets in train_ds:\n",
        "    for image, target in zip(images, targets):\n",
        "        # plt.figure(figsize=(15, 3))\n",
        "        # plt.imshow(1 - image)\n",
        "        # plt.show()\n",
        "        result, attention_plot = evaluate(image)\n",
        "        print(\"True:\", ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in target if i.numpy() != 0]))\n",
        "        print('Pred:', ' '.join(result))\n",
        "    break"
      ],
      "metadata": {
        "id": "wnibACxFrAIP",
        "outputId": "2dcbf072-fc55-4161-a31c-808c0b10c110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: <start> Số 459 phố Kim Ngưu Phường Vĩnh Tuy Quận Hai Bà Trưng Hà Nội <end>\n",
            "Pred: Số 459 phố Kim Ngưu Phường Vĩnh Tuy Quận Hai Bà Trưng Hà Nội <end>\n",
            "True: <start> Km 948600 Quốc lộ 1A Xã Điện Thắng Bắc Huyện Điện Bàn Quảng Nam <end>\n",
            "Pred: Km 948600 Quốc lộ 5A Xã Bạch Thắng Huyện Long Thành Quảng Nam <end>\n",
            "True: <start> Khu 7 Thị Trấn ái Nghĩa Huyện Đại Lộc Quảng Nam <end>\n",
            "Pred: Khu 7 Thị Trấn ái Nghĩa Huyện Đại Lộc Quảng Nam <end>\n",
            "True: <start> Số 117 Hùng Vương Phường Sở Dầu Quận Hồng Bàng Hải Phòng <end>\n",
            "Pred: Số 117 Hùng Vương Phường Sở Nghé Quận Hà Đông Hà Nội <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "\n",
        "    fig = plt.figure(figsize=(60, 12))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (3, 59))\n",
        "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(1 - image / tf.reduce_max(image))\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "iStYjWRGqrzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, targets in train_ds:\n",
        "    for image, target in zip(images, targets):\n",
        "        result, attention_plot = evaluate(image)\n",
        "        plot_attention(image, result, attention_plot)\n",
        "        print(\"True:\", ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in target if i.numpy() != 0]))\n",
        "        print('Pred:', ' '.join(result))\n",
        "    break"
      ],
      "metadata": {
        "id": "F667KQru8Glf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UDzvg-qfvpfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1619a4fb-411e-47c3-8735-850c870a6b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(true_texts)\n",
        "print(pred_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03UDv5Tj9zkn",
        "outputId": "98fc0fe4-2aa2-47a7-bb34-48ffb2bb1965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Số 10 đường Lý Văn Lâm Phường 1 Thành Phố Cà Mau Cà Mau', '271 Huỳnh Ngọc Huệ Phường Hoà Khê Quận Thanh Khê Đà Nẵng', 'Số [UNK] KP 9 Phường Hố Nai Thành Phố Biên Hoà Đồng Nai', 'Số [UNK] tổ 8 ấp Tân Hòa Xã Tân Tiến Huyện Bù Đốp Bình Phước']\n",
            "['Số nhà 71 phố 4 Phường Tân Tiến Thành phố Đồng Hới Quảng Trị', '729 âu Cơ Phường Thanh Khê Đông Quận Thanh Khê Đà Nẵng', 'Số 79 tổ 23 Phường Khương Đình Quận Cầu Giấy Hà Nội', 'Số 79 ngõ 328 Lê Lợi Xã Diên Khánh Huyện Diên Khánh Hà Nội']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cY2fAEUH9bAb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, target in zip(images, targets):\n",
        "    result, attention_plot = evaluate(image)\n",
        "    print(\"True:\", ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in target if i.numpy() != 0]))\n",
        "    print('Pred:', ' '.join(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYUai8Kn8OSX",
        "outputId": "9120a674-fe0d-44d9-c137-7faa291d17c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: <start> Khu 3 Xã Cát Quế Huyện Hoài Đức Hà Nội <end>\n",
            "Pred: Khu 3 Xã Cát Quế Huyện Hoài Đức Hà Nội <end>\n",
            "True: <start> Số 79 Đường số 37 Khu phố 2 Phường 10 Quận 6 TP Hồ Chí Minh <end>\n",
            "Pred: Số 79 Đường số 37 Khu phố 2 Phường 10 Quận 6 TP Hồ Chí Minh <end>\n",
            "True: <start> Số 25 Nguyễn Bỉnh Khiêm Phường 8 Thành phố Cà Mau Cà Mau <end>\n",
            "Pred: Số 25 Nguyễn Bỉnh Khiêm Phường 8 Thành phố Cà Mau Cà Mau <end>\n",
            "True: <start> 271/7B An Dương Vương Phường 03 Quận 5 TP Hồ Chí Minh <end>\n",
            "Pred: 271/7B An Dương Vương Phường 03 Quận 5 TP Hồ Chí Minh <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_captioning.ipynb#scrollTo=StQK3dgDcri0\n",
        "\n",
        "https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "\n",
        "https://keras.io/examples/nlp/semantic_similarity_with_bert/\n"
      ],
      "metadata": {
        "id": "Z5CJxAyXC_KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://arxiv.org/pdf/1703.09137.pdf\n",
        "2. https://viblo.asia/p/a-guide-to-image-captioning-part-1-gioi-thieu-bai-toan-sinh-mo-ta-cho-anh-gAm5yr88Kdb\n",
        "3. https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "4. https://arxiv.org/pdf/1502.03044.pdf\n",
        "5. https://keras.io/examples/vision/image_captioning/\n",
        "6. https://machinelearningmastery.com/the-bahdanau-attention-mechanism/\n",
        "7. https://keras.io/examples/audio/ctc_asr/"
      ],
      "metadata": {
        "id": "wMh5GUVjinie"
      }
    }
  ]
}