{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiedfbLDUvqH2+R8J9+Hm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LapTQ/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hungpham13/Vietnamese-HTR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_6PnrTXw-c",
        "outputId": "e046071f-911b-4e65-a916-bb65293e2597"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-HTR'...\n",
            "remote: Enumerating objects: 2403, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2403 (delta 0), reused 4 (delta 0), pack-reused 2399\u001b[K\n",
            "Receiving objects: 100% (2403/2403), 427.59 MiB | 24.04 MiB/s, done.\n",
            "Checking out files: 100% (2395/2395), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQiOo4_Fd7lh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0825_DataSamples 1/labels.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgkwHIEfYniD",
        "outputId": "a64be027-fea7-4a6d-f6a7-7f83a486dbb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"1.jpg\": \"Số 3 Nguyễn Ngọc Vũ, Hà Nội\",\n",
            "    \"2.jpg\": \"Số 30 Nguyên Hồng, Láng Hạ, Đống Đa, Hà Nội\",\n",
            "    \"3.jpg\": \"58 Thái Thịnh, Đống Đa, Hà Nội\",\n",
            "    \"4.jpeg\": \"Số 370/8 khu phố 5B, phường Tân Biên, Biên Hòa, Đồng Nai\",\n",
            "    \"5.jpg\": \"Vĩnh Trung Plaza, B, 255-257 đường Hùng Vương, phường Vĩnh Trung\",\n",
            "    \"6.jpg\": \"Tòa nhà 34T, Hoàng Đạo Thúy, Hà Nội\",\n",
            "    \"7.jpg\": \"40 Cát Linh, Đống Đa, Hà Nội\",\n",
            "    \"8.jpg\": \"phòng 101, tầng 1, lô 04-TT5B, khu đô thị Tây Nam Linh Đàm\",\n",
            "    \"9.JPG\": \"Nhà 87 ngõ 416 Đê La Thành\",\n",
            "    \"10.JPG\": \"Up coworking Space, 89 Láng Hạ, Hà Nội\",\n",
            "    \"11.jpg\": \"192 Ngô Đức Kế, quận 1, Hồ Chí Minh\",\n",
            "    \"12.jpg\": \"số 5 Công Trường Mê Linh, phường Bến Nghé, quận 1\",\n",
            "    \"13.jpg\": \"90A đường Mai Xuân Thưởng, tỉnh Gia Lai\",\n",
            "    \"14.jpg\": \"96/7/12B Phạm Văn Đồng, thành phố Pleiku\",\n",
            "    \"15.jpg\": \"168 Ngô Gia Tự, thành phố Hà Tĩnh\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0916_Data Samples 2'\n",
        "test_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/1015_Private Test'\n",
        "\n",
        "image_height, image_width = 120, 1900\n",
        "vocab_size = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "seq_length = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "embedding_dim = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "units = 512\n",
        "\n",
        "batch_size = 4\n",
        "epochs = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "mjm7Y-lYdZYT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of training images:', len(list(Path(train_img_dir).glob('*.png'))))\n",
        "print('Number of testing images:', len(list(Path(test_img_dir).glob('*.png'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8HNz-regz_",
        "outputId": "24a10ccc-eddc-481f-e5ff-9d696cec7ae4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 1823\n",
            "Number of testing images: 549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_json = json.load(\n",
        "    open(train_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "test_json = json.load(\n",
        "    open(test_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "train_data = {os.path.join(train_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in train_json.items()}\n",
        "test_data = {os.path.join(test_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in test_json.items()}"
      ],
      "metadata": {
        "id": "pQGy_mYAayjJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img_string = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img_string)\n",
        "\n",
        "    # resize to desired shape\n",
        "    # input is of int [0, 255], but output is of float [0, 255]\n",
        "    img = tf.image.resize_with_pad(img, image_height, image_width)\n",
        "\n",
        "    # invert color      ##############################\n",
        "    img = 255 - img\n",
        "\n",
        "    # preprocess_input accept input of type float [0, 255]\n",
        "    img = keras.applications.densenet.preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace('<', '')\n",
        "strip_chars = strip_chars.replace('>', '')\n",
        "strip_chars = strip_chars.replace('/', '')\n",
        "strip_chars = strip_chars.replace('-', '')\n",
        "\n",
        "vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length,\n",
        "    standardize=lambda label: tf.strings.regex_replace(label, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        ")\n",
        "\n",
        "vectorization.adapt(list(train_data.values()))\n",
        "\n",
        "def preprocess_input(img_path, label):\n",
        "    return decode_and_resize(img_path), vectorization(label)\n",
        "\n",
        "def make_dataset(img_paths, labels, training):\n",
        "    assert training is True or training is False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "    dataset = dataset.map(preprocess_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.prefetch(buffer_size=2000)\n",
        "    # dataset = dataset.cache()\n",
        "    if training: \n",
        "        dataset = dataset.shuffle(buffer_size=2000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = make_dataset(\n",
        "    train_data.keys(),\n",
        "    train_data.values(),\n",
        "    training = True\n",
        ")\n",
        "test_ds = make_dataset(\n",
        "    test_data.keys(),\n",
        "    test_data.values(),\n",
        "    training = False\n",
        ")"
      ],
      "metadata": {
        "id": "q0rEPI32rEMH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    for image, label in zip(images, labels):\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "        print(label)"
      ],
      "metadata": {
        "id": "e-bul7yM1PyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "7cf40a20-4d50-472b-95e6-39d3b0dba2f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAA4CAYAAAAGubjzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANJUlEQVR4nO2dbawcVRnHf8/u3r339rb2hSIQ5KUYNCF+kNIgH4CYqLyp4EtCSkyoaEJMJJEYQ2pIlI9Wgh+IRoKRAAYBjaLExAASo594KVjeKS0FIlhaKZXb0vu2u48f5szds9Od3ZndmZ3p9vklmzv37Lz855lznnPOc86eEVXFMAzDGC8qRQswDMMwssecu2EYxhhizt0wDGMMMeduGIYxhphzNwzDGEPMuRuGYYwhuTh3EblMRHaKyG4R2ZrHNQzDMIx4JOt57iJSBV4DvgC8DTwNXKOqL2d6IcMwDCOWPFru5wO7VXWPqi4CDwBX5XAdwzAMI4Y8nPupwL+9/992aYZhGMaIqBV1YRG5HrgeoFqtnjczM1OUFMMwjGOS2dnZ91T1xG7f5eHc3wFO8/7/mEvrQFXvBO4EEBGdnZ3NQYphGMZY81bcF3mEZZ4GzhaRDSJSBzYDD+dwHcMwDCOGzFvuqtoQkRuAR4AqcJeqvpT1dQzDMIx4Mp8KOZAIkeQiBChecnZUgk91JqgJFz9wyXWoz8D8wSLFGT1xz45G0UKM45hnVHVTty9K9wtVqUNlRTSxvbn6lJHKyZ8WgXOYhzUne8lLsPBBUaKMRLSA5mCHSp2gNl9O6LFzSUrp5Ep660xDYVM5jh9Kkm1YVqKLoJECI16GOvTfDK6VIoNW6jCxJoNr9qG5APt30damoK38r2vEUCNZ6RigFyk1qE5FE9OfpyeDluxq/FcLh8mu12x5O3fK49y9TKMLka+8jLByPcMVBAkKV1JaDVga1UQeBarp9B2TZOnIsnaKITmG/rQBjVmg6Tn5MI9XgInIAWkd4TCOfcCeSGJCbd3sO6juvPLAMU4pnXsvDr+ffN9lPGcpAiki/EHBSlq4hs1kCtRL0GKvkvxeBs1B9QGPi1BbPYSGXijpnGoam3lMrgaJhmdSPn+JtrYHOAfCSJxkbcZdJ/r8hZ69hp5MUCZPVhrKbZIurezWQvdde+IPeE1BK49WWcXFUQfFL1xFO/cWySvQQXKQAosDHNeFZlzlm1ZX1LGkfQYTDNTaXzhCh1OdXg1TCcOAUgWpwIrVnecYKP8osQPDUicbxy/QmHPb0R6CAkvpzrVMk+LLTAkptXOXCkyvItMWxcw02XU9PV2TK6GWVmfoUNwMoEodatEueVo9SSqYPjon15I8ZxQ4U0SqwHzMl2kL+zB5YogZM9oCf8JaswFLhxIe2wyO/zDamw23K5G/AzIxQ3ZhqgbDl2dvxpxUoZpRL3DcKIVzr8R0x7QJRw6SXcYS+PDw8OdYDvP4hVJgKc7RdKNGx+ApwLoTYXqIJ1KZgqkTPJ1Ce7qeT5w9K8Ex6z8KlZxzhvhjC4MW9mnQuNZeWv2VYJZWJU3l6nTXVjFwHtVI+GfxMDQz6tXUwxb9MOWnQmeL2u9hpn1unkPuRT3aE4k5D4BMBZMRuuo7zimFc2+1YGIlVKNTIHMg1hn0QCZoD3JN0LWV1hhk0DXSupxfgsNu+qP4zj/hU6pOQCNs9VVox417tWLDWGcYp1X4zxvBQHKeaCsYWGSCzpBI0hxZCe4XpV2J+XS55+pHYs7ljm8dCaagJkYDR1UbpjcYdb7D9Nx8atCcd+cexrm3ggonRGpBj3qoGDm9NTWO9P6+Q94cnc866Syn44BymEFhqgatNC3fhNSmvX8qDBSb0ybt1ktcq8plxmrS9c8aR2s5PNseTFW/0Pst3Mn4U06ucwUaujubbrFTdft6XfnoVNSedJvdEVKl7QC8nDYxDXVXkVemaVeWfq+oD1L1KjLfJuIG7YiM10hn+KODuHhzBWS6vd2ttNRmYMn/PULKmU5HNTYyChlKJWVFlRBdCvJHZcXgs7pEoeLnw0ieTKU7Wp4b5D/j5xihr3MXkbtEZL+IvOilrRORx0Rkl/u71qWLiNzu3sD0vIhsTCrk0P/ymSWyPIADgw+69DvOs2KzW8YMWzo9mJiBKd9JOWdTnYSKc0oTdVgR9m7C1rZ33vlDrjUc1zVdpH+LSEgVP65UoB6dsx0ySdt2ng1bwNJcMFBX93trKQZaK1VPp1dpS831OsKWvUcrYSx7mVq7oovrVeoiNF0ekzrpY+/R+82oxyRNmFrlep05UGuCDKi11Yj0DDMMu47Vr9eHJEnL/W7gskjaVuBxVT0beNz9D3A5cLb7XA/8MhuZCennREfx4Ls4p0q1/7VrK6Dh91zC+CTtSm9pAebDVmKXmUSNA0cfn5purZ4eNm01YDHOac51T27OBfdUnRgwnCWRuLS/rUEFJxOd+1T9HlxSltrnbi3QtZIPW6AiQeXblQL6x60mzB1MHoaUmJ5J504s98QW5/MP3fVDInqrWc3qGRP6ZjtV/SfwfiT5KuAet30P8BUv/V4NeAJYIyKjWzAgzRS+rK/b6+sEBWzuQMS5OxoLnWGSVnithiu43e43SxsMU1j6xHsbR1x8NS3Redyek9GGm0Xi2U1qvXUAQQimx8BznJNcdBWYKix9ePT3U2tganWfa5cATfJ7jjCEVxKiPX2x6ZAdDPpbyJNUda/bfhc4yW3HvYVpLxH8l3UYlHeebp6V5aDnTmkrbQRTDPudc9hbPSqmL7DYgFYXp29kTyOHMYZjmaE7jBosK5m6XKjqnaq6KW5FM2PMKMfQ/WhRaGW5HothpGDQlvs+ETlFVfe6sMt+l57oLUxRTj75ZLZs2TKgFMMwjOOTbdu2xX6XaD13ETkT+Iuqfsr9fytwQFV/IiJbgXWqepOIfBG4AbgC+Axwu6qe3+/8mzZt0u3btye4FcMwDCNERGLXc+/r3EXkfuCzwHpgH/Bj4E/A74DTCd7hd7Wqvi8iAvycYHbNEeA6Ve3rtUXkELAz6Q2VgPXAe0WLSIHpzRfTmy/Hml4YneYz4l6QXZY3MW0/lmLvpjdfTG++mN78KYPm43GYyzAMY+wx524YhjGGlMW531m0gJSY3nwxvflievOncM2liLkbhmEY2VKWlrthGIaRIYU7dxG5TER2upUkt/Y/Inc9p4nI30XkZRF5SUS+59JvEZF3RGSH+1zhHfNDp3+niFxakO43ReQFp227S8t89c6MtH7Ss+MOEZkVkRvLZOOsVkMVkS1u/10iktsv9WL03ioirzpND4nIGpd+pojMeXa+wzvmPJePdrt7ymUprhi9qZ//qPxHjN4HPa1visgOl164fQFQ1cI+BGvMvQ6cRbDa+HPAOQVrOgXY6LZXAa8B5wC3AD/osv85TvcksMHdT7UA3W8C6yNpPwW2uu2twDa3fQXwV4IluC4Aniw4D7wLnFEmGwMXAxuBFwe1J7AO2OP+rnXba0eo9xKg5ra3eXrP9PeLnOcpdw/i7unyEepN9fxH6T+66Y18fxvwo7LYV1ULb7mfD+xW1T2qugg8QLCyZGGo6l5VfdZtHwJeIVj8LI6rgAdUdUFV3wB2E9xXGSjn6p2dfA54XVXf6rHPyG2s2ayGeinwmKq+r6oHgcc4evns3PSq6qOqGi6Z9gTBciCxOM0fUdUnNPBE99K+x9z19iDu+Y/Mf/TS61rfVwP39zrHKO0LxYdl4laRLAUSLLtwLvCkS7rBdXHvCrvklOceFHhURJ6RYMVNSL96ZxFsprNQlNnGae1ZFt0A3yJoKYZsEJF/icg/ROQil3YqgcaQIvSmef5lse9FwD5V3eWlFW7fop17aRGRlcAfgBtVdZbgxSMfBz5NsITxbQXK68aFqrqR4IUp3xWRi/0vXUuhVFOjRKQOXAn83iWV3cbLlNGecYjIzQSr3t/nkvYCp6vqucD3gd+KSNwbZkfJMfP8I1xDZwOlFPYt2rkPtIpk3ojIBIFjv09V/wigqvtUtamqLeBXtMMCpbgHVX3H/d0PPESgb18YbpEMVu/MgcuBZ1V1H5TfxqS3Z+G6ReSbwJeAb7gKCRfeOOC2nyGIW3/CafNDNyPVO8DzL4N9a8DXgAfDtLLYt2jn/jRwtohscK24zcDDRQpy8bNfA6+o6s+8dD8m/VUgHDV/GNgsIpMisoHgFYNPjUqv0zYjIqvCbYKBtBedtnCGxhbgz57ma90sjwuAD7xwwyjpaPGU2caejjT2fAS4RETWuhDDJS5tJIjIZcBNwJWqesRLP1FEqm77LAJ77nGaZ0XkAlcOrvXucRR60z7/MviPzwOvqupyuKU09s1rpDbph2CmwWsEtdvNJdBzIUF3+3lgh/tcAfwGeMGlPwyc4h1zs9O/kxxHv3toPotgpsBzwEuhHYETCN5xuwv4G8HSzBCM1P/CaX4B2FSA5hngALDaSyuNjQkqnb0Eb1J9G/j2IPYkiHXvdp/rRqx3N0FMOszHd7h9v+7yyQ7gWeDL3nk2ETjV1wlWeJUR6k39/EflP7rpdel3A9+J7Fu4fVXVfqFqGIYxjhQdljEMwzBywJy7YRjGGGLO3TAMYwwx524YhjGGmHM3DMMYQ8y5G4ZhjCHm3A3DMMYQc+6GYRhjyP8BTO0X4wxJtxUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[   2  389  951 1065    4   61    5   16   14   10    9   11    8    3\n",
            "    0    0    0    0    0    0    0    0    0    0    0], shape=(25,), dtype=int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[   2 1711   23   19   95    7   19   45    6  604 1375   39   99  366\n",
            "    3    0    0    0    0    0    0    0    0    0    0], shape=(25,), dtype=int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  2  23  34  26  56  51  57   6  28 232  83 168  71 113 114   3   0   0\n",
            "   0   0   0   0   0   0   0], shape=(25,), dtype=int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  2 388  77 123 814   4 495 232   5  38  10   9  11   8   3   0   0   0\n",
            "   0   0   0   0   0   0   0], shape=(25,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 0\n",
        "# for label in train_data.values():\n",
        "#     print(\n",
        "#         tf.strings.regex_replace(label, \"[%s]\" % re.escape(\"!\\\"#$%&'()*+,.:;=?@[\\]^_`{|}~\"), \"\")\n",
        "#     )\n",
        "#     i += 1\n",
        "#     if i == 3:\n",
        "#         break\n",
        "    "
      ],
      "metadata": {
        "id": "Ge7AbaqSjC4k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorization.get_vocabulary()"
      ],
      "metadata": {
        "id": "ikaGA6DpISdR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(json_file.values())[0])\n",
        "# vectorization(list(train_data.values()))[:3]"
      ],
      "metadata": {
        "id": "39GqXt-fkxTd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.DenseNet121(\n",
        "    include_top=False,\n",
        "    input_shape=(image_height, image_width, 3),\n",
        "\n",
        ")\n",
        "# get the feature map from DenseNet\n",
        "base_model_out = base_model.output\n",
        "# squash the feature map from shape [f_height, f_width, f_channel]\n",
        "# to shape [f_height x f_width, f channel], we'll pass it through\n",
        "# a CNN Encoder later on\n",
        "base_model_out = keras.layers.Reshape(\n",
        "    (-1, base_model_out.shape[-1])\n",
        ")(base_model_out)\n",
        "\n",
        "cnn_model = keras.models.Model(\n",
        "    base_model.input,\n",
        "    base_model_out\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6bVPJQy5lcG",
        "outputId": "fbb8fdac-6ea2-442a-952b-66fabb226e20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "\n",
        "        self.feature_extractor = keras.applications.DenseNet121(\n",
        "            include_top=False,\n",
        "            input_shape=(image_height, image_width, 3),\n",
        "\n",
        "        )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # get the feature map from DenseNet\n",
        "        x = self.feature_extractor(x)\n",
        "        # squash the feature map from shape [f_height, f_width, f_channel]\n",
        "        # to shape [f_height x f_width, f channel]\n",
        "        x = keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
        "        # shape after fc == (f_height x f_width, embedding_dim)\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mbX4mMelNBzZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape = (batch_size, f_height x f_width, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # print('BahdanauAttention: hidden_with_time_axis.shape == ', hidden_with_time_axis.shape)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, f_height x f_width, units)\n",
        "        attention_hidden_layer = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis)\n",
        "        )\n",
        "\n",
        "        # score shape == (batch_size, f_height x f_width, 1)\n",
        "        # this gives an unnormalized score for each image feature\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # print('BahdanauAttention: score.shape == ', score.shape)\n",
        "\n",
        "        # attention_weights shape == (batch_size, f_height x f_width, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # print('BahdanauAttention: attention_weights.shape == ', attention_weights.shape)\n",
        "\n",
        "        # context vector shape after sum = (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        # print('BahdanauAttention: context_vector.shape == ', context_vector.shape)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "qhg43tIOYOzU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau vs Luong"
      ],
      "metadata": {
        "id": "yvghkvH3Ceqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # print('RNN_Decoder: context_vector.shape == ', context_vector.shape)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "wOpjbibZVo8U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vectorization.vocabulary_size())"
      ],
      "metadata": {
        "id": "EQd4P-TKAbQY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(trues, preds):\n",
        "    loss_ = loss_object(trues, preds)\n",
        "    \n",
        "    mask = tf.cast(\n",
        "        tf.math.logical_not(tf.math.equal(trues, 0)),\n",
        "        dtype=loss_.dtype\n",
        "    )\n",
        "\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "5uYOHU4EBIuu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints\n",
        "!mkdir checkpoints/train"
      ],
      "metadata": {
        "id": "7ecufyAFCYVI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoints/train'\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    optimizer=optimizer\n",
        ")\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    ckpt,\n",
        "    checkpoint_path,\n",
        "    max_to_keep=5\n",
        ")"
      ],
      "metadata": {
        "id": "CP6_XTeBCSyy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "metadata": {
        "id": "4ehXoIfNWUM0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot = []"
      ],
      "metadata": {
        "id": "8A1DmE6jgZUG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary())\n",
        "index_to_word = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "Q15fA47pj1Ak"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.variables import trainable_variables\n",
        "@tf.function\n",
        "def train_step(images, targets):\n",
        "    # print('train_step: images.shape == ', images.shape)\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=targets.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims(\n",
        "        [word_to_index('<start>')] * targets.shape[0],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(images)\n",
        "\n",
        "        for i in range(1, targets.shape[1]):\n",
        "            # print('train_step: dec_input.shape == ', dec_input.shape)\n",
        "            # print('train_step: features.shape == ', features.shape)\n",
        "            # print('train_step: hidden.shape == ', hidden.shape)\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "            loss += loss_function(targets[:, i], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targets[:, i], 1)\n",
        "    \n",
        "    total_loss = loss / targets.shape[1]\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss"
      ],
      "metadata": {
        "id": "PnvqHwevgb3J"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "D_5c1mhXrr_r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, (images, targets) in enumerate(train_ds):\n",
        "        batch_loss, t_loss = train_step(images, targets)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            average_batch_loss = batch_loss.numpy() / int(targets.shape[1])\n",
        "            print(f'Epoch: {epoch + 1} Batch: {batch} Loss: {average_batch_loss:.4f}')\n",
        "        \n",
        "    loss_plot.append(total_loss / train_ds.cardinality().numpy())\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} Loss: {total_loss/train_ds.cardinality().numpy():.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlBO4L8gpmQD",
        "outputId": "f10d9335-5626-4232-deb8-0529fbc418b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Batch: 0 Loss: 4.2038\n",
            "Epoch: 1 Batch: 50 Loss: 3.4650\n",
            "Epoch: 1 Batch: 100 Loss: 3.0933\n",
            "Epoch: 1 Batch: 150 Loss: 3.2055\n",
            "Epoch: 1 Batch: 200 Loss: 3.1392\n",
            "Epoch: 1 Batch: 250 Loss: 3.0756\n",
            "Epoch: 1 Batch: 300 Loss: 2.9405\n",
            "Epoch: 1 Batch: 350 Loss: 3.1981\n",
            "Epoch: 1 Batch: 400 Loss: 2.8714\n",
            "Epoch: 1 Batch: 450 Loss: 2.9510\n",
            "Epoch: 1 Loss: 3.018764\n",
            "Time taken for 1 epoch 442.96 sec\n",
            "\n",
            "Epoch: 2 Batch: 0 Loss: 2.8965\n",
            "Epoch: 2 Batch: 50 Loss: 2.5880\n",
            "Epoch: 2 Batch: 100 Loss: 2.6249\n",
            "Epoch: 2 Batch: 150 Loss: 2.6835\n",
            "Epoch: 2 Batch: 200 Loss: 2.5611\n",
            "Epoch: 2 Batch: 250 Loss: 2.5720\n",
            "Epoch: 2 Batch: 300 Loss: 2.2872\n",
            "Epoch: 2 Batch: 350 Loss: 2.2179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_captioning.ipynb#scrollTo=StQK3dgDcri0\n",
        "\n",
        "https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "\n",
        "https://keras.io/examples/nlp/semantic_similarity_with_bert/\n"
      ],
      "metadata": {
        "id": "Z5CJxAyXC_KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://arxiv.org/pdf/1703.09137.pdf\n",
        "2. https://viblo.asia/p/a-guide-to-image-captioning-part-1-gioi-thieu-bai-toan-sinh-mo-ta-cho-anh-gAm5yr88Kdb\n",
        "3. https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "4. https://arxiv.org/pdf/1502.03044.pdf\n",
        "5. https://keras.io/examples/vision/image_captioning/\n",
        "6. https://machinelearningmastery.com/the-bahdanau-attention-mechanism/\n"
      ],
      "metadata": {
        "id": "wMh5GUVjinie"
      }
    }
  ]
}