{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsV/PYQDbDi45EsXPYLaMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LapTQ/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hungpham13/Vietnamese-HTR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_6PnrTXw-c",
        "outputId": "50d315fa-ddb7-497c-a7bf-b3f4b3724360"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-HTR'...\n",
            "remote: Enumerating objects: 2403, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2403 (delta 0), reused 4 (delta 0), pack-reused 2399\u001b[K\n",
            "Receiving objects: 100% (2403/2403), 427.59 MiB | 38.26 MiB/s, done.\n",
            "Checking out files: 100% (2395/2395), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rQiOo4_Fd7lh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0825_DataSamples 1/labels.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgkwHIEfYniD",
        "outputId": "d57948fc-1abe-432f-ade1-df447d3b2ac6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"1.jpg\": \"Số 3 Nguyễn Ngọc Vũ, Hà Nội\",\n",
            "    \"2.jpg\": \"Số 30 Nguyên Hồng, Láng Hạ, Đống Đa, Hà Nội\",\n",
            "    \"3.jpg\": \"58 Thái Thịnh, Đống Đa, Hà Nội\",\n",
            "    \"4.jpeg\": \"Số 370/8 khu phố 5B, phường Tân Biên, Biên Hòa, Đồng Nai\",\n",
            "    \"5.jpg\": \"Vĩnh Trung Plaza, B, 255-257 đường Hùng Vương, phường Vĩnh Trung\",\n",
            "    \"6.jpg\": \"Tòa nhà 34T, Hoàng Đạo Thúy, Hà Nội\",\n",
            "    \"7.jpg\": \"40 Cát Linh, Đống Đa, Hà Nội\",\n",
            "    \"8.jpg\": \"phòng 101, tầng 1, lô 04-TT5B, khu đô thị Tây Nam Linh Đàm\",\n",
            "    \"9.JPG\": \"Nhà 87 ngõ 416 Đê La Thành\",\n",
            "    \"10.JPG\": \"Up coworking Space, 89 Láng Hạ, Hà Nội\",\n",
            "    \"11.jpg\": \"192 Ngô Đức Kế, quận 1, Hồ Chí Minh\",\n",
            "    \"12.jpg\": \"số 5 Công Trường Mê Linh, phường Bến Nghé, quận 1\",\n",
            "    \"13.jpg\": \"90A đường Mai Xuân Thưởng, tỉnh Gia Lai\",\n",
            "    \"14.jpg\": \"96/7/12B Phạm Văn Đồng, thành phố Pleiku\",\n",
            "    \"15.jpg\": \"168 Ngô Gia Tự, thành phố Hà Tĩnh\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0916_Data Samples 2'\n",
        "test_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/1015_Private Test'\n",
        "\n",
        "image_height, image_width = 120, 1900\n",
        "vocab_size = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "seq_length = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "embedding_dim = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "units = 512\n",
        "\n",
        "batch_size = 4\n",
        "epochs = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "mjm7Y-lYdZYT"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of training images:', len(list(Path(train_img_dir).glob('*.png'))))\n",
        "print('Number of testing images:', len(list(Path(test_img_dir).glob('*.png'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8HNz-regz_",
        "outputId": "3752f91a-4017-422d-9b9f-7086de0d87b9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 1823\n",
            "Number of testing images: 549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_json = json.load(\n",
        "    open(train_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "test_json = json.load(\n",
        "    open(test_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "train_data = {os.path.join(train_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in train_json.items()}\n",
        "test_data = {os.path.join(test_img_dir, image_name): '<start> ' + label + ' <end>' for image_name, label in test_json.items()}"
      ],
      "metadata": {
        "id": "pQGy_mYAayjJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img_string = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img_string)\n",
        "\n",
        "    # resize to desired shape\n",
        "    # input is of int [0, 255], but output is of float [0, 255]\n",
        "    img = tf.image.resize_with_pad(img, image_height, image_width)\n",
        "\n",
        "    # preprocess_input accept input of type float [0, 255]\n",
        "    img = keras.applications.densenet.preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace('<', '')\n",
        "strip_chars = strip_chars.replace('>', '')\n",
        "strip_chars = strip_chars.replace('/', '')\n",
        "strip_chars = strip_chars.replace('-', '')\n",
        "\n",
        "vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length,\n",
        "    standardize=lambda label: tf.strings.regex_replace(label, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        ")\n",
        "\n",
        "vectorization.adapt(list(train_data.values()))\n",
        "\n",
        "def preprocess_input(img_path, label):\n",
        "    return decode_and_resize(img_path), vectorization(label)\n",
        "\n",
        "def make_dataset(img_paths, labels, training):\n",
        "    assert training is True or training is False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "    dataset = dataset.map(preprocess_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.prefetch(buffer_size=2000)\n",
        "    # dataset = dataset.cache()\n",
        "    if training: \n",
        "        dataset = dataset.shuffle(buffer_size=2000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = make_dataset(\n",
        "    train_data.keys(),\n",
        "    train_data.values(),\n",
        "    training = True\n",
        ")\n",
        "test_ds = make_dataset(\n",
        "    test_data.keys(),\n",
        "    test_data.values(),\n",
        "    training = False\n",
        ")"
      ],
      "metadata": {
        "id": "q0rEPI32rEMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49998e7-6b55-4b19-dd90-1cf797e08a06"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_ds.take(1):\n",
        "#     for image, label in zip(images, labels):\n",
        "#         plt.imshow(image)\n",
        "#         plt.show()\n",
        "#         print(label)"
      ],
      "metadata": {
        "id": "e-bul7yM1PyY"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 0\n",
        "# for label in train_data.values():\n",
        "#     print(\n",
        "#         tf.strings.regex_replace(label, \"[%s]\" % re.escape(\"!\\\"#$%&'()*+,.:;=?@[\\]^_`{|}~\"), \"\")\n",
        "#     )\n",
        "#     i += 1\n",
        "#     if i == 3:\n",
        "#         break\n",
        "    "
      ],
      "metadata": {
        "id": "Ge7AbaqSjC4k"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorization.get_vocabulary()"
      ],
      "metadata": {
        "id": "ikaGA6DpISdR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(json_file.values())[0])\n",
        "# vectorization(list(train_data.values()))[:3]"
      ],
      "metadata": {
        "id": "39GqXt-fkxTd"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.DenseNet121(\n",
        "    include_top=False,\n",
        "    input_shape=(image_height, image_width, 3),\n",
        "\n",
        ")\n",
        "# get the feature map from DenseNet\n",
        "base_model_out = base_model.output\n",
        "# squash the feature map from shape [f_height, f_width, f_channel]\n",
        "# to shape [f_height x f_width, f channel], we'll pass it through\n",
        "# a CNN Encoder later on\n",
        "base_model_out = keras.layers.Reshape(\n",
        "    (-1, base_model_out.shape[-1])\n",
        ")(base_model_out)\n",
        "\n",
        "cnn_model = keras.models.Model(\n",
        "    base_model.input,\n",
        "    base_model_out\n",
        ")"
      ],
      "metadata": {
        "id": "a6bVPJQy5lcG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "\n",
        "        self.feature_extractor = keras.applications.DenseNet121(\n",
        "            include_top=False,\n",
        "            input_shape=(image_height, image_width, 3),\n",
        "\n",
        "        )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # get the feature map from DenseNet\n",
        "        x = self.feature_extractor(x)\n",
        "        # squash the feature map from shape [f_height, f_width, f_channel]\n",
        "        # to shape [f_height x f_width, f channel]\n",
        "        x = keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
        "        # shape after fc == (f_height x f_width, embedding_dim)\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mbX4mMelNBzZ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape = (batch_size, f_height x f_width, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # print('BahdanauAttention: hidden_with_time_axis.shape == ', hidden_with_time_axis.shape)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, f_height x f_width, units)\n",
        "        attention_hidden_layer = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis)\n",
        "        )\n",
        "\n",
        "        # score shape == (batch_size, f_height x f_width, 1)\n",
        "        # this gives an unnormalized score for each image feature\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # print('BahdanauAttention: score.shape == ', score.shape)\n",
        "\n",
        "        # attention_weights shape == (batch_size, f_height x f_width, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # print('BahdanauAttention: attention_weights.shape == ', attention_weights.shape)\n",
        "\n",
        "        # context vector shape after sum = (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        # print('BahdanauAttention: context_vector.shape == ', context_vector.shape)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "qhg43tIOYOzU"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau vs Luong"
      ],
      "metadata": {
        "id": "yvghkvH3Ceqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # print('RNN_Decoder: context_vector.shape == ', context_vector.shape)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "wOpjbibZVo8U"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vectorization.vocabulary_size())"
      ],
      "metadata": {
        "id": "EQd4P-TKAbQY"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(trues, preds):\n",
        "    loss_ = loss_object(trues, preds)\n",
        "    \n",
        "    mask = tf.cast(\n",
        "        tf.math.logical_not(tf.math.equal(trues, 0)),\n",
        "        dtype=loss_.dtype\n",
        "    )\n",
        "\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "5uYOHU4EBIuu"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints\n",
        "!mkdir checkpoints/train"
      ],
      "metadata": {
        "id": "7ecufyAFCYVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14848f13-48f1-495c-9251-5c595c6da82f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n",
            "mkdir: cannot create directory ‘checkpoints/train’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoints/train'\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    optimizer=optimizer\n",
        ")\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    ckpt,\n",
        "    checkpoint_path,\n",
        "    max_to_keep=5\n",
        ")"
      ],
      "metadata": {
        "id": "CP6_XTeBCSyy"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "metadata": {
        "id": "4ehXoIfNWUM0"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot = []"
      ],
      "metadata": {
        "id": "8A1DmE6jgZUG"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary())\n",
        "index_to_word = keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=vectorization.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "Q15fA47pj1Ak"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.variables import trainable_variables\n",
        "@tf.function\n",
        "def train_step(images, targets):\n",
        "    # print('train_step: images.shape == ', images.shape)\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=targets.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims(\n",
        "        [word_to_index('<start>')] * targets.shape[0],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(images)\n",
        "\n",
        "        for i in range(1, targets.shape[1]):\n",
        "            # print('train_step: dec_input.shape == ', dec_input.shape)\n",
        "            # print('train_step: features.shape == ', features.shape)\n",
        "            # print('train_step: hidden.shape == ', hidden.shape)\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "            loss += loss_function(targets[:, i], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targets[:, i], 1)\n",
        "    \n",
        "    total_loss = loss / targets.shape[1]\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss"
      ],
      "metadata": {
        "id": "PnvqHwevgb3J"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "id": "D_5c1mhXrr_r"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, (images, targets) in enumerate(train_ds):\n",
        "        batch_loss, t_loss = train_step(images, targets)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            average_batch_loss = batch_loss.numpy() / int(targets.shape[1])\n",
        "            print(f'Epoch: {epoch + 1} Batch: {batch} Loss: {average_batch_loss:.4f}')\n",
        "        \n",
        "    loss_plot.append(total_loss / train_ds.cardinality().numpy())\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} Loss: {total_loss/train_ds.cardinality().numpy():.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "metadata": {
        "id": "AlBO4L8gpmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss / train_ds.cardinality().numpy()"
      ],
      "metadata": {
        "id": "aqrat_Zb8Rm7",
        "outputId": "e5137b23-d1b6-42c4-a19b-dd40d0bf30f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3.1029227>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_captioning.ipynb#scrollTo=StQK3dgDcri0\n",
        "\n",
        "https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "\n",
        "https://keras.io/examples/nlp/semantic_similarity_with_bert/\n"
      ],
      "metadata": {
        "id": "Z5CJxAyXC_KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://arxiv.org/pdf/1703.09137.pdf\n",
        "2. https://viblo.asia/p/a-guide-to-image-captioning-part-1-gioi-thieu-bai-toan-sinh-mo-ta-cho-anh-gAm5yr88Kdb\n",
        "3. https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "4. https://arxiv.org/pdf/1502.03044.pdf\n",
        "5. https://keras.io/examples/vision/image_captioning/\n",
        "6. https://machinelearningmastery.com/the-bahdanau-attention-mechanism/\n"
      ],
      "metadata": {
        "id": "wMh5GUVjinie"
      }
    }
  ]
}