{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP18z6JADoecOo1Lk/Nyykk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LapTQ/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hungpham13/Vietnamese-HTR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_6PnrTXw-c",
        "outputId": "ce7b2abd-14e8-45e2-a2d9-42143b8a77ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-HTR'...\n",
            "remote: Enumerating objects: 2403, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2403 (delta 0), reused 4 (delta 0), pack-reused 2399\u001b[K\n",
            "Receiving objects: 100% (2403/2403), 427.59 MiB | 31.24 MiB/s, done.\n",
            "Checking out files: 100% (2395/2395), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQiOo4_Fd7lh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0825_DataSamples 1/labels.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgkwHIEfYniD",
        "outputId": "5e59621a-b58d-48fd-a94c-e182c58367cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"1.jpg\": \"Số 3 Nguyễn Ngọc Vũ, Hà Nội\",\n",
            "    \"2.jpg\": \"Số 30 Nguyên Hồng, Láng Hạ, Đống Đa, Hà Nội\",\n",
            "    \"3.jpg\": \"58 Thái Thịnh, Đống Đa, Hà Nội\",\n",
            "    \"4.jpeg\": \"Số 370/8 khu phố 5B, phường Tân Biên, Biên Hòa, Đồng Nai\",\n",
            "    \"5.jpg\": \"Vĩnh Trung Plaza, B, 255-257 đường Hùng Vương, phường Vĩnh Trung\",\n",
            "    \"6.jpg\": \"Tòa nhà 34T, Hoàng Đạo Thúy, Hà Nội\",\n",
            "    \"7.jpg\": \"40 Cát Linh, Đống Đa, Hà Nội\",\n",
            "    \"8.jpg\": \"phòng 101, tầng 1, lô 04-TT5B, khu đô thị Tây Nam Linh Đàm\",\n",
            "    \"9.JPG\": \"Nhà 87 ngõ 416 Đê La Thành\",\n",
            "    \"10.JPG\": \"Up coworking Space, 89 Láng Hạ, Hà Nội\",\n",
            "    \"11.jpg\": \"192 Ngô Đức Kế, quận 1, Hồ Chí Minh\",\n",
            "    \"12.jpg\": \"số 5 Công Trường Mê Linh, phường Bến Nghé, quận 1\",\n",
            "    \"13.jpg\": \"90A đường Mai Xuân Thưởng, tỉnh Gia Lai\",\n",
            "    \"14.jpg\": \"96/7/12B Phạm Văn Đồng, thành phố Pleiku\",\n",
            "    \"15.jpg\": \"168 Ngô Gia Tự, thành phố Hà Tĩnh\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/0916_Data Samples 2'\n",
        "test_img_dir = '/content/Vietnamese-HTR/Data 1: Handwriting OCR for Vietnamese Address/1015_Private Test'\n",
        "\n",
        "image_height, image_width = 120, 1900\n",
        "vocab_size = 10000\n",
        "seq_length = 25\n",
        "embedding_dim = 512\n",
        "ff_dim = 512\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "mjm7Y-lYdZYT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of training images:', len(list(Path(train_img_dir).glob('*.png'))))\n",
        "print('Number of testing images:', len(list(Path(test_img_dir).glob('*.png'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8HNz-regz_",
        "outputId": "6ea3240a-d701-4deb-c9f7-1a00ab19a74c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 1823\n",
            "Number of testing images: 549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_json = json.load(\n",
        "    open(train_img_dir + '/labels.json', 'r')\n",
        ")\n",
        "\n",
        "test_json = json.load(\n",
        "    open(test_img_dir + '/labels.json', 'r')\n",
        ")"
      ],
      "metadata": {
        "id": "pQGy_mYAayjJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max([len(label.split()) for label in json_file.values()])\n",
        "# tokens = set()\n",
        "# for label in json_file.values():\n",
        "#     for token in label.split():\n",
        "#         tokens.add(token)\n",
        "# len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMNBoqXifjvV",
        "outputId": "8a9b566f-58f8-45cf-bf44-bd8cab7a36b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2494"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img_string = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img_string)\n",
        "\n",
        "    # resize to desired shape\n",
        "    # input is of int [0, 255], but output is of float [0, 255]\n",
        "    img = tf.image.resize_with_pad(img, image_height, image_width)\n",
        "\n",
        "    # preprocess_input accept input of type float [0, 255]\n",
        "    img = keras.applications.densenet.preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length,\n",
        "    standardize=lambda label: tf.strings.regex_replace(label, \"[%s]\" % re.escape(\"!\\\"#$%&'()*+,.:;<=>?@[\\]^_`{|}~\"), \"\")\n",
        ")\n",
        "\n",
        "vectorization.adapt(list(train_json.values()))\n",
        "\n",
        "def preprocess_input(img_path, label):\n",
        "    return decode_and_resize(img_path), vectorization(label)\n",
        "\n",
        "def make_dataset(img_paths, labels, training):\n",
        "    assert training is True or training is False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "    dataset = dataset.map(preprocess_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.prefetch(buffer_size=2000)\n",
        "    dataset = dataset.cache()\n",
        "    if training: \n",
        "        dataset = dataset.shuffle(buffer_size=2000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = make_dataset(\n",
        "    [str(path) for path in sorted(list(Path(train_img_dir).glob('*.png')))],\n",
        "    train_json.values(),\n",
        "    training = True\n",
        ")\n",
        "test_ds = make_dataset(\n",
        "    [str(path) for path in sorted(list(Path(test_img_dir).glob('*.png')))],\n",
        "    test_json.values(),\n",
        "    training = False\n",
        ")"
      ],
      "metadata": {
        "id": "q0rEPI32rEMH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    for image, label in zip(images, labels):\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "        print(label)"
      ],
      "metadata": {
        "id": "e-bul7yM1PyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label in train_json.values():\n",
        "    print(\n",
        "        tf.strings.regex_replace(label, \"[%s]\" % re.escape(\"!\\\"#$%&'()*+,.:;<=>?@[\\]^_`{|}~\"), \"\")\n",
        "    )\n",
        "    break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge7AbaqSjC4k",
        "outputId": "3df313e7-34f3-472f-f49b-ea00ea2b6877"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'S\\xe1\\xbb\\x91 253 \\xc4\\x91\\xc6\\xb0\\xe1\\xbb\\x9dng Tr\\xe1\\xba\\xa7n Ph\\xc3\\xba Th\\xe1\\xbb\\x8b tr\\xe1\\xba\\xa5n Nam S\\xc3\\xa1ch Huy\\xe1\\xbb\\x87n Nam S\\xc3\\xa1ch H\\xe1\\xba\\xa3i D\\xc6\\xb0\\xc6\\xa1ng', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorization.get_vocabulary()"
      ],
      "metadata": {
        "id": "ikaGA6DpISdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(json_file.values())[0])\n",
        "# vectorization(list(json_file.values())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39GqXt-fkxTd",
        "outputId": "2ee990cf-2a38-4986-9ad9-6dc70863ac50"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số 253 đường Trần Phú, Thị trấn Nam Sách, Huyện Nam Sách, Hải Dương\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(25,), dtype=int64, numpy=\n",
              "array([ 11, 875,  46,  75,  20,  18,  39,  37, 369,   4,  37, 369,  32,\n",
              "        52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.DenseNet121(\n",
        "    include_top=False,\n",
        "    input_shape=(image_height, image_width, 3),\n",
        "\n",
        ")\n",
        "# get the feature map from DenseNet\n",
        "base_model_out = base_model.output\n",
        "# squash the feature map from shape [f_height, f_width, f_channel]\n",
        "# to shape [f_height x f_width, f channel], we'll pass it through\n",
        "# a CNN Encoder later on\n",
        "base_model_out = keras.layers.Reshape(\n",
        "    (-1, base_model_out.shape[-1])\n",
        ")(base_model_out)\n",
        "\n",
        "cnn_model = keras.models.Model(\n",
        "    base_model.input,\n",
        "    base_model_out\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6bVPJQy5lcG",
        "outputId": "c761580b-14f9-49bf-e312-96ccf7280dc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n",
            "29097984/29084464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, f_height x f_width, embedding_dim)\n",
        "        self.fc = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mbX4mMelNBzZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape = (batch_size, f_height x f_width, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, f_height x f_width, units)\n",
        "        attention_hidden_layer = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis)\n",
        "        )\n",
        "\n",
        "        # score shape == (batch_size, f_height x f_width, 1)\n",
        "        # this gives an unnormalized score for each image feature\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # attention_weights shape == (batch_size, f_height x f_width, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context vector shape after sum = (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n"
      ],
      "metadata": {
        "id": "qhg43tIOYOzU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau vs Luong"
      ],
      "metadata": {
        "id": "yvghkvH3Ceqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "wOpjbibZVo8U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://arxiv.org/pdf/1703.09137.pdf\n",
        "2. https://viblo.asia/p/a-guide-to-image-captioning-part-1-gioi-thieu-bai-toan-sinh-mo-ta-cho-anh-gAm5yr88Kdb\n",
        "3. https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "4. https://arxiv.org/pdf/1502.03044.pdf\n",
        "5. https://keras.io/examples/vision/image_captioning/\n",
        "6. https://machinelearningmastery.com/the-bahdanau-attention-mechanism/\n"
      ],
      "metadata": {
        "id": "wMh5GUVjinie"
      }
    }
  ]
}